{
    "contents" : "---\ntitle: \"Introduction to Natural Language Processing - Do Good Data 2016\"\nauthor:\n  name: Dan Acheson\n  affiliation: Timshel\n  email: dan@timshel.com\noutput: \n  html_document:\n    toc: true\n---\n\n\n***\n\nDependencies\n========================================================\n### Code written in R 3.1.3__\n### The code below should install what you don't already have__\n```{r dependencies}\nsetwd(getwd())\nrequired_packages <- c('stringr', 'stringi', 'quanteda', 'tm', 'data.table', 'topicmodels', 'e1071', 'wordcloud', 'RColorBrewer', 'proxy', 'ape', 'servr')\n\nnew.packages <- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\n\nif(length(new.packages)){\n  install.packages(required_packages, repos='http://cran.us.r-project.org')\n}\n```\n\n***\n\nNatural Language Processing \n=====\n- Techniques, tools and algorithms that will allow computational approaches to language processing\n- Historically comes from the combination of a number of fields\n    - Computer science\n    - Computational linguistics\n    - Artificial Intelligence\n    - Psychology\n\n\n***\n\n## Terminology\n- __Natural language:__ language produced by humans\n- __Text__: the writen form of language\n- __String__: computer representation of text (a.k.a., character) \n- __Document__: a bunch of text that comes from the same place\n    - Can be as large as a whole book, or as small as a single tweet\n- __Corpus__: A collection of documents (plural: _corpora_)\n\n***\n\n## Data Sources for Text\n__See additional resources below for more__\n\n1. APIs from the services you love\n    - [Twitter API](https://dev.twitter.com/overview/documentation) is particularly is useful\n2. Web-scraping\n3. PDFs / printed text\n    - Need optical character recognition (OCR)\n4. Digitized collections of books, newspapers, etc.\n    - [Project Gutenberg](https://www.gutenberg.org/)\n    - [New York Times](http://www.nytimes.com/ref/membercenter/nytarchive.html)\n5. Curated corpora by NLP researchers\n6. [Wikipedia!](http://https://dumps.wikimedia.org/)\n\n***\n\nText Wrangling 101\n========================================================\n- Very important skills to learn if you're going to work with text\n- Base R has some decent functionality\n\n>```grep gsub substr strsplit tolower/toupper paste/paste0```\n\n- **To start:** UPPER and lower case\n```{r echo = F}\nsetwd(getwd())\n```\n```{r upper_lower}\nmyText <- \"Do Good Data 2016\"\ntolower(myText) #lowercase\ntoupper(myText) #uppercase\n```\n\n***\n\n## String Splitting\n- Today we'll primarily use the [```stringr```](https://cran.r-project.org/package=stringr) package\n- Faster and more transparent than base R\n- Uses the [```stringi```](https://cran.r-project.org/package=stringi) package underneath\n\n**Comma separated text**\n```{r str_split1}\nlibrary(stringr)\nmyText <- \"Comma,separated,text\"\nstr_split(myText,\",\")\n```\n\n**Email addresses**\n```{r str_split2}\nmyText <- \"dan@timshel.com\"\nstr_split(myText,\"@\")\n```\n\n***\n\n## String Splitting on a list\n**Applying ```str_split``` on a list**\n```{r str_split_list}\nlist_of_emails <- c(\"fred@flinstones.com\", \"wilma@flinstones.com\",\"pebbles@flinstones.com\")\nsplit_list <- sapply(list_of_emails, function(x) str_split(x,\"@\"), USE.NAMES =F)\nsplit_list[1:2]\n\n#Extract names from list\nfirst_names <- sapply(split_list, function(x) x[1])\nlast_names <- sapply(split_list, function(x) x[2])\nfirst_names\n```\n\n***\n\n## Combining / concatenating text\n**Using base R's ```paste``` or ```paste0```**\n```{r paste}\nsplit_text = c(\"My\", \"text\", \"in\", \"a\", \"vector / list\")\nsplit_text\npaste0(split_text, collapse = \",\")\n```\n\n***\n\n## Trimming\n**Remove trailing or leading whitespace (e.g., a newline) with ```str_trim```**\n```{r str_trim}\nmyText = \"  Space at beginning removed\"\nstr_trim(myText, side = \"left\")\n```\n```{r}\nmyText = \"Text with carriage return and newlines removed\\r\\n\"\nstr_trim(myText, side = \"right\")\n```\n\n***\n\nRegular Expressions\n========================================================\n**A brief but important aside**\n\n- Regular Expressions (i.e., regexes) are simple yet powerful language for finding patterns in text\n- If you don't know 'em, go out and learn 'em. \n- You'll find tons of applications and seriously increase your ability to search, munge and manipulate text.\n- For more info, check the following: [regex in stringi](http://finzi.psych.upenn.edu/library/stringi/html/stringi-search-regex.html)\n\n## Regular Expression Syntax:\nSyntax | Example\n------ | -------\n`[ ]` set membership <br><br> | `[A-Za-z]` = all letters <br><br>\n`[^ ]` set exclusion <br><br> | `[^0-9]` = exclude numbers <br><br>\n`*` repeat pattern 0 or more times <br><br> | `[a-z]*` = lowercase repeat 1+<br><br>\n`+` repeat pattern 1 or more times <br><br> | `ab+` = \"ab\" repeated 0+ times<br><br>\n`{1,3}` repeats between 1-3X <br><br> | `[a-z]{1,3}` = lowercase letters 1-3X<br><br>\n`\\` escape character to ignore regex syntax (`\\\\` in R) | `\\\\.com` = treat   \" . \"   like a period\n`.` anything once | `b.b` matches \"bob\", \"bub\" but not \"barb\"\n`.*` anything repeated (greedy) <br>**Be Careful!**  `.*?` is non-greedy | `b\\.*b` matches anything beginning and ending with \"b\"\n`^` begins with | `^[A-Z]` = begins with capital letter\n`$` ends with | `.*ing$` = anything ending with \"ing\"\n\n\n***\n\nWrangling Continued\n========================================================\n## Substituting / Replacing\n**Getting rid of numbers**\n```{r regex replace, cache = T}\nmyText = \"Text with 9879numbers 54 I do43n't wan234t\"\nstr_replace_all(myText,\"[0-9]+\",\"\")\n#Alternate version\nstr_replace_all(myText,\"[:digit:]\",\"\")\n```\n\n***\n\n## Extracting Text\n**Extract email addresses!**\n```{r regex_extract1, cache = T}\nmyText = \"Address 1: dan@timshel.com, Address 2: barney_rubble@flinstones.com\"\nstr_extract_all(myText, \"[a-zA-Z0-9_]+@.*.[a-zA-Z]+\", simplify = T)\n```\n**There are two mistakes above. Can you spot them?**\n\n### Extracting Email - Correct\n**Remember ```.*``` is _greedy_, so be careful!**\n**Here using ```.*?``` and escaping the . with ```\\\\.```**\n```{r regex_extract2, cache = T}\n#NOTE: In R, escape is \\\\\nstr_extract_all(myText, \"[a-zA-Z0-9_]+@.*?\\\\.[a-zA-Z]+\", simplify = T)\n```\n\n\n### Extracting HTML content \n**Using the ```str_match``` function**\n```{r regex_match, cache = T}\nhtml <- paste(readLines(\"./pres_data/example.html\"), collapse=\"\\n\")\nregex = \"<li>(.*?)</li>\"\nstr_match_all(html, regex)\n```\n\n***\n\n## Why learn all of this manual text-processing?\n- Forms of the basis of text processing we'll see later\n- Gives you the tools to customize!\n- Can quickly get to word frequencies, which can be VERY informative\n\n\n***\n\nWord Frequencies\n========================================================\n## Example\n- Let's look at frequencies from 'Alice in Wonderland'\n- Data comes from [Project Gutenberg](https://www.gutenberg.org/)\n\n**Load and Pre-Process**\n```{r word_freq1, cache = T}\nalice <- \"./pres_data/alice.txt\"\n#read into a character vector\nalice <- paste(readLines(alice), collapse = \"\\n\")\n#Simple pre-processing\npre_process <- function(txt) {\n  txt <- tolower(txt)\n  txt <- str_replace_all(txt, \"\\n\", \" \") #newline with space\n  txt <- str_replace_all(txt,\"[ ]{2,}\", \" \") #extra spaces\n  txt <- str_replace_all(txt, \"[^a-zA-Z ]\",\"\") #only keep letters\n  return(txt)\n}\n#Pre-process\nalice <- pre_process(alice)\n\n#Get Frequencies\nwords <- str_split(alice,\" \")\nfreqs <- table(words)\nfreqs <- freqs[order(freqs,decreasing = T)]\nfreqs[1:8]\n```\n**What do you notice?**\n\n***\n\n## Plotting word frequencies with wordclouds\n**Using the [```wordcloud```](https://cran.r-project.org/web/packages/wordcloud/) package**\n**We _COULD_ use our frequency table...**\n\n```{r wordcloud, fig.align='center', cache = T}\nlibrary(wordcloud)\nwordcloud(words = names(freqs), freq = as.numeric(freqs), max.words = 20, scale = c(4,0.5))\n```\n\n**Or, we can just take advantage of the package :)**\n\n```{r wordcloud2, fig.align='center', fig.width =5, fig.height = 5, cache = T}\nwc2 = wordcloud(alice, max.words = 20, scale = c(4,0.5))\n```\n\n\n## Wordclouds with color!\n**Use two visual dimensions to emphasize frequency differences**\n```{r wordcloud3, fig.align='center', warning = F, cache = T}\nlibrary(RColorBrewer)\npal <- brewer.pal(8,\"Dark2\")\n\nwordcloud(alice, max.words = 20, scale = c(4,0.5), color = pal)\n```\n\n\n***\n\n## Why all this focus on word frequencies?\n1. They're informative!\n2. They are the basis of:\n\n[__THE VECTOR-SPACE MODEL__](https://en.wikipedia.org/wiki/Vector_space_model)\n<div align = \"center\">\n<img src=\"./pres_data/images/Vector_Space.png\" width=1000 height=600>\n</div>\n\n***\n\nThe Vector Space / N-Gram Model\n========================================================\n**Representing documents as vectors**\n```\ntext 1: The cow jumped over the moon.\ntext 2: The cow ate grass.\n```\n\n```{r dtm example, echo = F, message = F, cache = T}\nlibrary(quanteda) #we'll get into the quanteda package more momentarily\ntxt  = c(\"The cow jumped over the moon.\", \"The cows ate grass\")\ntxt_corpus = corpus(txt)\ndoc_mat = dfm(txt_corpus, clean = F, verbose = F) \n```\n\n**The document feature or document term matrix**\n```{r dtm example 2,echo =F, cache = T}\ndoc_mat\n```\n\n***\n\n## Pre-processing text for vector space model\n**Pre-processing depends on what you're doing. But generally:**\n1. Remove punctuation\n2. [Tokenization](https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis\\)\n3. Remove stop-words (e.g., \"a\", \"the\", etc.)\n4. Lowercase\n5. Remove low-frequency words\n6. [Stemming](https://en.wikipedia.org/wiki/Stemming)\n\n***\n\n### Tokenization\n**How you break up your text into meaningful units for analysis**\n<br>\n- Could be individual words\n  - Unigram / bag of words representation\n\n- Could be pairs or triples of words\n  - bigram / trigram\n\n- Could be sentences, paragraphs, etc.\n\n***\n\n### Word stemming\n- Treat similar forms of a word:\n_run_, _runs_, _running_, _runner_?\n\n- As the same word:\n_run_\n\n**Reasons for doing it:**\n\n- Reduce vocabulary size\n- Increase similarity across texts\n\n**Reasons to not do it:**\n\n- Lose information\n\n#### Word stemming - main types\n\n1. Remove suffixes from words\n_run_, _runs_, _running_, _runner_ -> run\n\n- Common algorithms:\n    - Porter, Snowball, Lancaster, regexp\n\n2. Lemmatization\n- Use syntactic analysis to remove inflectional (i.e., sytnactic) endings\n_am_, _is_, _are_ -> be\n\n\n***\n\nVectorizing documents\n========================================================\n- Thankfully, all of this pre-processing is built into packages / libraries that handle text!\n- Today we'll use the [```quanteda```](https://cran.r-project.org/package=quanteda) package\n```{r dtm_example_clean, message = F, cache = T}\nlibrary(quanteda)\ntxt  = c(\"The cow jumped over the moon.\", \"The cows ate grass\")\n#Convert text list to a corpus\ntxt_corpus = corpus(txt)\ndoc_mat2 = dfm(txt_corpus, clean = T, stem = T, ignoredFeatures = stopwords(\"english\"), verbose = F)\ndoc_mat2\n``` \n\n## Effects of pre-processing\n**No Pre-Processing**\n```{r doc_mat1, echo = F, cache = T}\ndoc_mat\n```\n\n**With Pre-Processing**\n```{r doc_mat2, echo = F, cache = T}\ndoc_mat2\n```\n\n***\n\n## From unigrams to n-grams\n- So far what we've done is a unigram, or bag-of-words model\n- Can also incorporate bigrams\n```{r bigrams, echo = F, cache = T}\ndoc_mat_bg = dfm(txt_corpus, clean = T, stem = T, ignoredFeatures = stopwords(\"english\"), verbose = F, bigrams = T)\ndoc_mat_bg\n```\n- If you want to move to trigrams, check out the [```tm```](https://cran.r-project.org/package=tm) package\n- __Food for thought:__ _What happens to our matrix as we add bigrams, trigrams, etc?_\n\n\n***\n\nNormalizing a document-term matrix\n========================================================\n\n## What happens if documents are different lengths?\n```{r read_alice_again, echo = F, cache = T}\nalice <- \"./pres_data/alice.txt\"\n#read into a character vector\nalice <- paste(readLines(alice), collapse = \"\\n\")\n```\n\n**A corpus of Alice in Wonderland and our single sentence used before**\n```{r length_compare, cache = T}\nlibrary(quanteda)\ntxt  = c(\"The cow jumped over the moon.\", alice)\ntxt_corpus = corpus(txt)\ndoc_mat3 = dfm(txt_corpus, clean = T, stem = F, ignoredFeatures = stopwords(\"english\"), verbose = F)\nsort(doc_mat3)[,1:10]\n``` \n\n***\n\n## Normalization techniques\n- Normalize by frequency within each document (i.e., row of our document-term matrix)\n- There are many ways you could do this\n\n### Relative frequency\n```{r relFreq, cache = T}\nrelFreq = weight(doc_mat3, type = \"relFreq\")\nsort(relFreq)[,1:5]\n```\n\n### Maximum frequency\n```{r relMaxFreq, cache = T}\nrelFreq = weight(doc_mat3, type = \"relMaxFreq\")\nsort(relFreq)[,1:5]\n```\n\n- Other common options:\n    - **natural log**\n    - **length normalization**\n\n***\n\n## Term Frequency Inverse Document Frequency (TFIDF)\n**A go-to approach for many NLP tasks**\n1. Normalize by frequency within each document (i.e., by row)\n2. Normalize by _inverse_ frequency across documents (i.e., by column)\n  - ```-log(1 / (# of docs term appears in))```\n  \n### Some insight into what's going on\n__The effect of TFIDF is to emphasize words that frequent within a document, and infrequent across documents__\n<div align=\"center\", position = \"center\">\n<img src=\"./pres_data/images/Term_Frequency.png\" width = 500>\n</div>\n\n<div align=\"center\", position = \"center\">\n<img src=\"./pres_data/images/TFIDF.png\" width = 500>\n</div>\n\n\n\n```{r tfidf, cache = T}\ntfidf_norm = weight(doc_mat2, type = \"tfidf\")\nsort(tfidf_norm)[,1:5]\n```\n\n\n***\n\nMachine Learning with Text\n========================================================\n**After all the vectorizing, we end up with a matrix of features to use for many different tasks:**\n\n- __Unsupervised learning:__\n    * clustering\n    * document similarity\n    * semantic analysis\n- __Supervised learning:__\n    * sentiment analysis\n    * document classification / tagging\n- __Information retrieval__\n\n***\n\n## Clustering / Document Similarity\n__Answers the question:__ Which of our documents group together based the language they contain?\n\nBasic Procedure:\n\n1. Clean and vectorize text\n2. Create a distance / similarity matrix (not always)\n3. Cluster\n\n***\n\n### Clustering Example: Tweets about sustainable development\n**The data:**\nDownloaded from the Twitter search API looking for _\"sustainable development\"_ and _\"SDG\"_\n\n```{r read_tweets_sdg, cache = T}\ntweets = read.csv(\"./pres_data/twitter_dat.csv\", stringsAsFactors = F)\nprint(tweets$text[1])\ndim(tweets)\n#Let's remove retweets\nRTs <- sapply(tweets$text, function(x) str_detect(x, \"RT \"))\ntweets <- tweets[!(RTs),]\ndim(tweets)\n```\n\nThe ```clean tweets``` function below is a modification from [this blogpost](\"https://sites.google.com/site/miningtwitter/questions/sentiment/viralheat\")\n```{r clean_tweets1, cache = T}\nclean_tweets <- function(txt) {\n  #Args:\n  #  txt: character vector of text from twitter\n  \n  #Returns:\n  # txt:  cleaned character vector of text\n  \n  # remove retweet entities\n  txt = str_replace_all(txt, \"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\")\n  # remove at people\n  txt = str_replace_all(txt,\"@\\\\w+\", \"\")\n  # remove html links\n  txt = str_replace_all(txt,\"http\\\\S+\", \" \")\n  # remove punctuation\n  txt = str_replace_all(txt,\"[[:punct:]]\", \" \")\n  # remove numbers\n  txt = str_replace_all(txt, \"[[:digit:]]\", \" \")\n  # remove unnecessary spaces\n  txt = str_replace_all(txt,\"[ \\t]{2,}\", \" \")\n  txt = str_trim(txt,\"both\")\n  # remove single character words remaining\n  txt = str_replace_all(txt, \" [a-zA-Z] \",\" \")\n  txt = tolower(txt)\n  return(txt)\n}\n```\n\n#### Clean the tweets\n```{r apply_clean_tweets, cache = T}\ntweets$text_clean = sapply(tweets$text, function(x) clean_tweets(x))\n#Uncleaned\nprint(tweets$text[1:2])\n#Cleaned\nprint(tweets$text_clean[1:2])\n```\n\n#### Vectorizing for clustering\n- Choice of how you vectorize will depend on the simililarity metric you use\n    - frequency or tfidf coded  \n    - binary coding\n\n- Some choices during pre-processing:\n    - stem?\n    - removed words\n    - frequency cutoffs\n\n```{r tweet_quanteda, cache = T}\ntweet_corpus = corpus(tweets$text_clean) \ntweet_dfm = dfm(tweet_corpus, clean = T, verbose = F,\n                ignoredFeatures = c(stopwords(\"english\"),\"sustainable\",\"development\",\"goals\"))\ndim(tweet_dfm)\n#remove terms with low frequency across documents\ntweet_dfm = trim(tweet_dfm, minDoc = 40, verbose = T)\ntweet_dfm = weight(tweet_dfm, method = \"tfidf\")\n```\n\n***\n\n#### Calculate distance / similarity\n- Many different ways of calculating distance / similarity:\n    - Euclidean\n    - Jaccard\n    - Cosine\n  \n- Distance calculations can take a while depending on the size of the matrix.\n- Solutions:\n    - Reduce the size of the matrix\n    - Consider trying to parallelize\n\n- The [proxy](\"https://cran.r-project.org/web/packages/proxy/index.html\") package has a number of distance / similarity measures available\n\n- Here we do Euclidean distance after scaling each column\n    - This is roughly equivalent to cosine similarity\n```{r distance_calculation, cache = T}\nlibrary(proxy) \n#Only perform on a subset to save some time\ntweet_dfm_sub = tweet_dfm[1:3000,]\nsim_mat = dist(scale(tweet_dfm_sub))\n```\n\n***\n\n### Hierarchical Clustering\n- Challenges:\n  - where to make the cuts / how many clusters?\n  - how to [visualize](http://rpubs.com/gaston/dendrograms)?\n\n**Perform hierarchical clustering**\n```{r hier_clust, cache = T}\nhier_clust = hclust(sim_mat, method=\"ward.D\")\n```\n\n***\n\n#### Visualizing Hierarchical Clusters: Dendrograms\nPart of the reason for plotting this is to look for how things are grouping together\n- This should give you insight into how many clusters you might try to extract or where you might make cuts\n**From base R** ```hclust```\n```{r hier_clust_plot, cache = T}\nplot(hier_clust, labels = F) \n```\n\n**Fan plot** from [ape](\"https://cran.r-project.org/web/packages/ape/index.html\") package\n\n```{r hier_clust_fan, cache = T}\nlibrary(ape)\nplot(as.phylo(hier_clust), type = \"fan\") \n```\n\n***\n\n#### Visualizing Clusters of Text with Wordclouds\n**Steps:**\n\n1. Assign documents to a group by cutting the tree\n2. Look at wordclouds for each group\n```{r hier_clust_word_clouds, cache = T,fig.align='center', eval = T}\n#Cut into 20 groups\ngroups = cutree(hier_clust, k = 20)\n\n#Plot wordclouds\npar(mfrow = c(3,3))\nfor(group in seq(1,17,2)){\n  group_dat = tweet_dfm_sub[which(groups==group),]\n  plot(group_dat, max.words = 20, scale = c(2,0.2))\n}\n```\n\n***\n\n### KMeans Clustering\n\n- One advantage of using KMeans: No similarity matrix needed!\n- Still need to think about:\n    - preprocessing\n    - choice of K\n    - visualizing\n\n```{r kmeans, cache = T}\nk = 20 \nkmeans_clust = kmeans(tweet_dfm[1:3000,], centers = k)\ngroups = kmeans_clust$cluster\n```\n\n#### Plotting KMeans\n```{r kmeans_word_clouds2, cache = T,fig.align='center', eval = T, echo = F, warning = F}\npar(mfrow = c(4,4))\n \nfor(group in 1:16){\n  group_dat = tweet_dfm_sub[which(groups==group)]\n  plot(group_dat, max.words = 20)\n}\n```\n\n***\n\n## Semantic Analysis through topic modeling\n__Answers the question:__ What is the underlying meaning captured in different documents?\n\n- [Topic modeling](https://en.wikipedia.org/wiki/Topic_model) is a statistical approach to natural language processing that find patterns in documents where words within the document can be assigned to abstract 'topics'\n- A document about sports is likely to have a different distribution of words than a document about cooking\n- Check out material from [David Blei](https://www.cs.princeton.edu/~blei/topicmodeling.html) for good intros\n\n### Approaches to topic modeling\n\n**[Latent Semantic Indexing (LSI) / Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_indexing)**\n- uses singular value decomposition (SVD) over a document-term matrix\n- equivalent to multivariate principle components analysis (PCA)\n\n**[Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)**\n- a generative (i.e, probabilistic) framework for estimating topics \n- assumes each document is a mixture of topics defined by the words in the document\n- most commonly used today\n\n***\n\n### Latent Dirichlet Allocation (LDA)\n#### Algorithm\n- Initialization\n    - Choose the number of topics \n    - Randomly assign topics to words within a document according to a dirichlet distribution\n    - Gives you distributio of words over topics and distribution of topics over documents\n  \n- Learning - iteratively repeat the following:\n  For each document calculate:\n    - Probabality of each topic given the words in the document\n    - For each word, the probability of the different topics based on the topic assignment of that word in the other documents\n\n***\n\n#### Text pre-processing for LDA\n- LDA works off of raw term frequencies. So, tfidf is out here.\n\n- Preprocessing:\n    - lowercase, punctuation and stop-word removal\n    - tokenization\n    - stemming\n    - Removing low frequency words\n        - document occurence\n        - using tfidf cutoff\n\n***\n\n### LDA over DoGoodData session descriptions\n- Let's have some fun with DoGoodData's data!\n- Data is a webscrape of the session description\n- This is a toy example, and is __not__ the right data for topic modeling\n    - there aren't enough documents\n    - text is sometimes not very long\n  \n- All that being said...today we'll use the [```topicmodels```](https://cran.r-project.org/web/packages/topicmodels/index.html) library\n\n**Create a document-term matrix as before**\n```{r LDA1a, eval = T, cache = T}\nlibrary(topicmodels)\ndgd_txt = textfile(\"./pres_data/DoGoodData_sessions/*.txt\", cache = F)\ndgd_corpus1 = corpus(dgd_txt)\ndgd_dfm1 = dfm(dgd_corpus1, clean = T, verbose = F,\n              ignoredFeatures = stopwords(\"english\"))\npaste0(\"Matrix dimensions: \", paste(dim(dgd_dfm1), collapse = \" X \"))\n\n```\n**Run the topic model with 10 topics**\n```{r LDA1a1, eval = T, cache = T}\ndgd_LDA10 <- LDA(convert(dgd_dfm1, to = \"topicmodels\"), k = 10)\n\n```\n\n***\n\n#### LDA1 - Results\nView top 5 terms for each document\n```{r LDA1c, cache = T}\nget_terms(dgd_LDA10,5)\n```\nWhat do you think?\n\n***\n\n#### \"Improving\" our LDA\nLots of repeated terms across topics\n```{r LDA2a, cache = T}\ntopfeatures(dgd_dfm1)\n```\nDefinitely too many topics given our corpus size\n```{r LDA2b, cache = T}\ntop_remove =  names(topfeatures(dgd_dfm1)[1:8])\ndgd_dfm = dfm(dgd_corpus, clean = T, verbose = F, # had to use dgd_corpus1\n              ignoredFeatures = c(top_remove,stopwords(\"english\")))\n#run with 4 topics only\ndgd_LDA4<- LDA(convert(dgd_dfm, to = \"topicmodels\"), k = 4)\nget_terms(dgd_LDA4,5)\n```\n- __NOTE:__ removing terms as below is not typical for topic modeling\n- It's done here to illustrate the differerences between topics\n\n*** \n\n#### Visualizing Topic Models - wordclouds\n- Wordclouds here came from topic models run on nonprofit mission statements located in their 990 tax forms\n- Words are top X words for a topic, Size correspond to the weight.\n\n![alt text](./pres_data/images/topic_27.png)\n![alt text](./pres_data/images/topic_52.png)\n\n***\n\n#### Visualizing Topic Models - networks\n- Nodes are words, edges are associations to the same topic\n<div align=\"center\">\n<img src=\"./pres_data/images/semantic_network.png\" width=900 height=700>\n</div>\n\nimage comes from [tetne tutorial](http://diging.github.io/tethne/api/tutorial.mallet.html)\n\n***\n\n#### Visualizing Topic Models with LDAvis\n- [LDAvis](https://github.com/cpsievert/LDAvis): a fantastic tool for both R and Python\n- Trick is to get your topic model data in the right format\n- Here, I modified some code from [Christopher Gandrud](http://christophergandrud.blogspot.com/2015/05/a-link-between-topicmodels-lda-and.html) (hidden below)\n```{r ldaVis_code, echo = T, cache = T}\n#' Convert the output of a topicmodels Latent Dirichlet Allocation to JSON\n#' for use with LDAvis\n#'\n#' @param fitted Output from a topicmodels \\code{LDA} model.\n#' @param corpus Corpus object used to create the document term\n#' matrix for the \\code{LDA} model. This should have been create with\n#' the tm package's \\code{Corpus} function.\n#' @param doc_term The document term matrix used in the \\code{LDA}\n#' model. This should have been created with the quanteda's\n#' \\code{dtm} function.\n#'\n#' @seealso \\link{LDAvis}.\n#' @export\n\ntopicmodels_json_ldavis <- function(fitted, corpus, doc_term){\n    # Required packages\n    library(topicmodels)\n    library(quanteda)\n    library(LDAvis)\n\n    # Find required quantities\n  \n    phi <- as.matrix(posterior(fitted)$terms) #a matrix with the topic-term distributions\n    theta <- as.matrix(posterior(fitted)$topics) #a matrix with the document-topic distributions\n    #vocabulary used to fit model\n    vocab = doc_term@Dimnames$features \n    #summary of corpus, used to get token count for each document\n    corpus_summary = summary(corpus, verbose = F)\n    doc_length = corpus_summary$Tokens \n    \n    term_frequency = doc_term@p[2:length(doc_term@p)] #first element is 0\n\n    # Convert to json\n    json_lda <- LDAvis::createJSON(phi = phi, theta = theta,\n                            vocab = vocab,\n                            doc.length = doc_length,\n                            term.frequency = term_frequency)#freq_matrix$Freq)\n\n    return(json_lda)\n}\n```\n**Process data and serve it up**\n```{r ldaVis, message = F, cache = T}\n#Visualizing the original topic model\nlda_vis_dat <- topicmodels_json_ldavis(fitted = dgd_LDA10, corpus = dgd_corpus1, doc_term = dgd_dfm1)\n# This should open a viewer in your RStudio session or a browser window\nserVis(lda_vis_dat)\n```\n\n*** \n\n### How do you know if your model is right?\nThis is hard to answer directly for unsupervised learning tasks\n- __Do the topics make sense?__\n    - Use your own judgment for this. There is no 'correct' answer.\n\n\n- __[Perplexity](http://qpleple.com/perplexity-to-evaluate-topic-models/) on a hold-out set of documents__\n    - How \"confused\" is the model with it's predictions for the words in the documents\n\n\n- __Can the results be used for another task?__ E.g., supervised learning\n    - In the end of topic modeling or clustering you've reduced your high-dimensional matrix into a much lower-dimensional representation\n        - Could be a cluster assignment for a given document\n        - Could be the topic loadings\n    - Does this new way of representing the data help in a supervised learning task? If so, then you're on to something.\n\n*** \n\n## Supervised learning with text\nUsing our vectorized text as features to learn about a known outcome variable\n**Today:**\n - Sentiment Analysis\n - Document Classification\n \nCommonly-used algorithms:\n- Naive Bayes\n- Multinomial logistic regression (a.k.a. maximum entropy; maxent)\n- Support Vector Machines\n\nThings that don't work so well:\n- Ensemble techniques like:\n  - Random Forests\n  - Gradient Boosting Machines\n\n***\n\n### Sentiment Analysis\n__Answers the question:__ What is the emotional valence of a particular piece of text?\nOld school:\n- create a dictionary of positive / negative terms\n- count frequency for each text source\n- majority count wins or difference/ratio of positive to negative\n\nThrough the lens of machine learning:\n- Positive / negative valence can be seen as an outcome variable\n- This simply amounts to a supervised learning problem:\n  - __Categorization:__ Sentiment classified as positive, negative or neutral\n  - __Regression:__ Sentiment varies continuously from negative to positive\n- Categorization is more frequent\n\n\n#### Classifying sentiments of Tweets\n- Data comes from the [twitter sentiment corpus](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/)\n- Each tweet is categorized as negative (\"0\") or positive (\"1\")\n\n```{r read_twit_sent}\nlibrary(data.table)\ntwit_dat = fread(\"./pres_data/Twitter_Sentiment_Data.csv\")\ndim(twit_dat)\nnames(twit_dat)\ntwit_dat$SentimentText[1:5]\n```\n\n\n#### Data preprocessing for twitter sentiment\n```{r clean_twit_sent, cache = T}\ntwit_dat$text_clean <- clean_tweets(twit_dat$SentimentText)\n\ntweet_corpus = corpus(twit_dat$text_clean) \ntweet_dfm = dfm(tweet_corpus, clean = T, verbose = F,\n                ignoredFeatures = c(stopwords(\"english\")))\ndim(tweet_dfm)\n#remove terms with low frequency across documents\ntweet_dfm = trim(tweet_dfm, minDoc = 200, verbose = T)\ntweet_dfm = weight(tweet_dfm, method = \"tfidf\")\n\n```\n\n#### Train a Naive Bayes classifier\nUsing the ```e1071``` library\n(this might take a minute or two to complete)\n```{r naive_bayes, cache = T}\nlibrary(e1071)\n\n#Split into train and test\ntrain_x = tweet_dfm[1:40000]\ntest_x = tweet_dfm[40001:50000]\ntrain_y = twit_dat$Sentiment[1:40000]\ntest_y = twit_dat$Sentiment[40001:50000]\n\nnb = naiveBayes(x = as.matrix(train_x), y = as.factor(train_y))\npreds = predict(nb, newdata = as.matrix(test_x), type = 'class')\n```\n\n#### Model Performance\n**Accuracy**\n```{r nb1 accuracy}\naccuracy = mean(preds == as.factor(test_y))\nprint(paste0(\"Accuracy: \", accuracy))\n```\n\n**Confusion Matrix**\n```{r nb1 conf_mat}\nconf_mat = table(preds, as.factor(test_y), dnn= c(\"pred\",\"actual\"))\nprint(\"Confusion Matrix:\")\nconf_mat\n```\n\n\n*** \n#### How could we improve our model?\n**Think about the _features_ being used here**\n\n- \"@apostropheme i'm a real BOY goddamit!!!!!!!!!!!!!! guh. apostro. i feel sad. the library lady thinks i'm stupid. SHE'S STUPID.  j\"       \n- \"@Abigailjune92 Weeeeeelllllll helllllllo abbiie! No one ever tells me they have twitter and I've had it for ages. Hope you're not too ill\"\n- \"awwwwwwwwwwwww that is so beautiful. I just need to be in his arms tonight\"\n\n\n#### Modified data cleaning and preprocessing a bit\n- Keep punctuation\n```{r, eval = T}\ntxt = \"i'm a real BOY goddamit!!!!!!!!!!!!!!\"\ntxt = str_replace_all(txt,\"([:punct:])\",\" \\\\1 \")\nprint(txt)\n```\n\n- Normalize repeated letters\n``` {r, eval = T}\ntxt = \"awwwwwwwwwwwww that is so beautiful.\"\ntxt = str_replace_all(txt,\"([a-z])\\\\1+\",\"\\\\1\\\\1\")\nprint(txt)\n```\n\n- Keep stopwords\n  - they include 'not'\n\n- Stem\n  - to reduce some variation in wordforms\n\n```{r clean_tweets2, cache = T, echo = T}\nclean_tweets2 <- function(txt) {\n  #Args:\n  #  txt: character vector of text from twitter\n  \n  #Returns:\n  # txt:  cleaned character vector of text\n  \n  # remove retweet entities\n  txt = str_replace_all(txt, \"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\")\n  # remove at people\n  txt = str_replace_all(txt,\"@\\\\w+\", \"\")\n  # remove html links\n  txt = str_replace_all(txt,\"http\\\\S+\", \" \")\n  # keep punctuation but add a space\n  txt = str_replace_all(txt,\"([:punct:])\",\" \\\\1 \")\n  # remove numbers\n  #txt = str_replace_all(txt, \"[[:digit:]]\", \" \")\n  # remove unnecessary spaces\n  txt = str_replace_all(txt,\"[ \\t]{2,}\", \" \")\n  txt = str_trim(txt,\"both\")\n\n  #Normalize repeat letters\n  txt = str_replace_all(txt,\"([a-z])\\\\1+\",\"\\\\1\\\\1\")\n  \n  txt = tolower(txt)\n  return(txt)\n}\n```\n\n***\n\n#### Naive Bayes2\n**Pre-Processing as described above**\n```{r clean_twit_sent2, cache = T, echo = T}\ntwit_dat$text_clean <- clean_tweets2(twit_dat$SentimentText)\n\ntweet_corpus = corpus(twit_dat$text_clean) \ntweet_dfm = dfm(tweet_corpus, clean = T, verbose = F, stem = T)\n#remove terms with low frequency across documents\ntweet_dfm = trim(tweet_dfm, minDoc = 200, verbose = T)\ntweet_dfm = weight(tweet_dfm, method = \"tfidf\")\n\n```\n\n**Train model as before**\n```{r naive_bayes2, cache = T, echo = T}\nlibrary(e1071)\n\n#Split into train and test\ntrain_x = tweet_dfm[1:40000]\ntest_x = tweet_dfm[40001:50000]\ntrain_y = twit_dat$Sentiment[1:40000]\ntest_y = twit_dat$Sentiment[40001:50000]\n\nnb = naiveBayes(x = as.matrix(train_x), y = as.factor(train_y))\npreds = predict(nb, newdata = as.matrix(test_x), type = 'class')\n```\n\n#### Model 2 Performance\n**Accuracy**\n```{r nb2 accuracy}\naccuracy2 = mean(preds == as.factor(test_y))\ncat(paste0(\"Accuracy1: \", accuracy, \"\\nAccuracy2: \", accuracy2))\n```\n**Confusion Matrix**\n```{r nb2 conf_mat}\nconf_mat2 = table(preds, as.factor(test_y), dnn= c(\"pred\",\"actual\"))\nconf_mat2\n```\n\n- Well, that's a pretty modest improvement over the first model, but, sentiment is hard!\n\n\n#### Some food for thought re: sentiment analysis\n- As with any analysis: _Garbage in, Garbage out_\n  - some of our tweets aren't very good, nor is the scoring\n  \n- We're high-dimensional here: _More data = better_\n- Lot's of nuance to sentiment that's hard to learn, e.g., sarcasm\n\n***\n\n### Document classification\n__Answers the question:__ What category should this document be assigned to?\n\n**Examples:**\n - Categorize emails according to a known system\n - Tag comment section / tweets / websites according to some taxonomy\n \n- From a machine learning standpoint, this is the same thing as sentiment analysis using a categorical outcome\n- Often multi-category / multi-nomial\n- Otherwise, same principles apply!\n\n***\n\nMoving forward\n==============\n## Explore other ways of vectorizing text:\n(both easier in Python)\n\n1. [Hashing](https://en.wikipedia.org/wiki/Feature_hashing) vectorizers (available in [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html))\n2. [word2vec](https://en.wikipedia.org/wiki/Word2vec) and doc2vec (available in [gensim](https://radimrehurek.com/gensim/models/doc2vec.html))\n\n***\n\nWe've only scratched the surface of NLP!\n========================================================\n__- Syntactic analysis__\n\n  - Part-of-speech (POS) tagging\n  - Syntactic parsing\n\n__- Named entity recognition__\n\n  - Treating people/places/proper nouns as a single entity\n\n__- Machine translation__\n\n  - E.g., Google translate\n\n__- Speech recognition__\n\n  - Combines all of the above and more\n\n__- Natural language generation__\n\n  - algorithms that can produce language\n\n\n---\n\nAdditional Resources\n========================================================\n## Regex\n- [Decent cheatsheet](http://www.rexegg.com/regex-quickstart.html)\n- [Useful Quora Response](http://stackoverflow.com/questions/4736/learning-regular-expressions)\n- [RegexOne](http://regexone.com/) - tutorial walkthrough\n\n## Libraries for Text Processing\n### R\n- Check out [Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) on CRAN\n- The [tm](https://cran.r-project.org/web/packages/tm/index.html) provides lots of NLP functionality\n- [quanteda](https://cran.r-project.org/web/packages/quanteda/index.html)\n\n### Python\n- [NLTK](http://www.nltk.org/) - lots of NLP functionality \n- [gensim](https://radimrehurek.com/gensim/) - fantastic for topic modeling, vectorizing and beyond\n- [Pattern](http://www.clips.ua.ac.be/pattern) for NLP and web scraping\n\n### Free NLP libraries, cross platform or Python\n- [openNLP](https://opennlp.apache.org/)\n- [Stanford Core NLP](http://stanfordnlp.github.io/CoreNLP/)\n- [Stanford Part of Speech (POS) tagger](http://nlp.stanford.edu/software/tagger.shtml)\n- [Stanford Named Entity Recognition (NER)](http://nlp.stanford.edu/software/CRF-NER.shtml)\n\n---\n\n## Web-scraping\n### R\n- [Selenium](https://github.com/ropensci/RSelenium) - web crawling by simulating a browser\n- [Rcurl](https://cran.r-project.org/web/packages/RCurl/index.html) - web requests\n- [rvest](https://github.com/hadley/rvest) for DOM parsing\n\n### Python\n- [Selenium](http://selenium-python.readthedocs.io/) - web crawling\n- [Scrapy](http://scrapy.org/) - web crawling\n- [Beautiful Soup](https://pypi.python.org/pypi/beautifulsoup4) - web crawling + DOM parsing\n\n---\n\n## Twitter\n### R\n- [twitteR](https://cran.r-project.org/web/packages/twitteR/index.html)\n\n### Python\n- [python-twitter](https://github.com/bear/python-twitter)\n- [Tweepy](http://www.tweepy.org/)\n\n---\n\n## Data sources + Corpora\n- [Wikipedia Dump](https://en.wikipedia.org/wiki/Wikipedia:Database_download) - all of wikipedia in multiple languages\n- [UCI machine learning repository - text](http://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table)\n\n- [Penn Treebank](https://www.cis.upenn.edu/~treebank/) - grammatically annotated text\n\n### News\n- [Reuters](http://about.reuters.com/researchandstandards/corpus/)\n- [New York Times](https://catalog.ldc.upenn.edu/LDC2008T19)\n\n### Multilingual\n- [Europarl](http://www.statmt.org/europarl/) - translation of European parliament between many languages\n- [OPUS](http://opus.lingfil.uu.se/) - multilingual corpus of translated text\n- [DCEP](https://ec.europa.eu/jrc/en/language-technologies/dcep) - Digital corpus of the European Parliament\n\n### Lists of corpora\n- [Wikipedia](https://en.wikipedia.org/wiki/List_of_text_corpora)\n- [BYU.edu](http://corpus.byu.edu/)\n- [Lancaster.uk](http://www.lancaster.ac.uk/staff/xiaoz/papers/corpus%20survey.htm)\n- [UOW.edu.au](http://www.uow.edu.au/~dlee/corpora.htm)\n- [Center for Research in Language](http://crl.ucsd.edu/corpora/)\n",
    "created" : 1472230170189.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "786594742",
    "id" : "99FDE3F7",
    "lastKnownWriteTime" : 1471878974,
    "path" : "~/Desktop/post_exCog/Intro_NLP_DoGoodData2016.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_markdown"
}